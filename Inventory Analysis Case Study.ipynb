{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5c0389",
   "metadata": {},
   "source": [
    "# Inventory Analysis Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e88cc",
   "metadata": {},
   "source": [
    "Inventory data analysis involves examining and interpreting\n",
    "data related to a company's inventory to gain insights, make\n",
    "informed decisions, and optimize inventory management\n",
    "processes.\n",
    "\n",
    "Python with libraries like Pandas, Matplotlib, and Seaborn is\n",
    "commonly used for this type of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb57491",
   "metadata": {},
   "source": [
    "# Inventory Optimization and Sustainability Analysis\n",
    "\n",
    "Objectives:\n",
    "\n",
    "    Minimize waste by optimizing inventory levels.\n",
    "    Identify the most efficient purchasing strategies based on sales, procurement, and inventory data.\n",
    "    Evaluate product sales performance to formulate a sustainable inventory management approach.\n",
    "\n",
    "1. Data Preprocessing:\n",
    "\n",
    "    Consolidate all CSV files into a unified master dataset.\n",
    "    Screen for any missing or erroneous entries.\n",
    "    Standardize date formats for consistent time series analysis.\n",
    "\n",
    "2. Inventory Analysis:\n",
    "\n",
    "    Assess inventory status at the year's start and end using BegInvFINAL12312016.csv and EndInvFINAL12312016.csv.\n",
    "    Pinpoint products with the highest and lowest inventory presence.\n",
    "\n",
    "3. Sales Analysis:\n",
    "\n",
    "    Examine SalesFINAL12312016.csv to identify bestsellers and products with sluggish sales.\n",
    "    Analyze sales trends over time, considering variables such as sales quantity, sales price, and date.\n",
    "\n",
    "4. Purchasing Analysis:\n",
    "\n",
    "    Evaluate procurement activities using PurchasesFINAL12312016.csv and InvoicePurchases12312016.csv.\n",
    "    Investigate purchase volumes from different suppliers, procurement costs, and supply chain processes.\n",
    "\n",
    "5. Optimal Stock Level Calculation:\n",
    "\n",
    "    Determine the optimal stock level for each product by leveraging sales, procurement, and inventory data.\n",
    "    Propose stock levels tailored to the sales velocity of products and supply lead times.\n",
    "\n",
    "Conclusion:\n",
    "The insights derived from these analyses will provide recommendations for managing inventory in a more efficient and sustainable manner, aiming to cut costs and prevent overstocking and waste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b57ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7896acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading datasets\n",
    "purchase_prices = pd.read_csv(r\"C:\\Users\\HP\\Desktop\\Intern\\Alfido tech\\task_3\\PurchasesFINAL12312016.csv\")\n",
    "beg_inv = pd.read_csv(r'C:\\Users\\HP\\Desktop\\Intern\\Alfido tech\\task_3\\BegInvFINAL12312016.csv')\n",
    "end_inv = pd.read_csv(r'C:\\Users\\HP\\Desktop\\Intern\\Alfido tech\\task_3\\EndInvFINAL12312016.csv')\n",
    "invoice_purchases = pd.read_csv(r'C:\\Users\\HP\\Desktop\\Intern\\Alfido tech\\task_3\\InvoicePurchases12312016.csv')\n",
    "purchases = pd.read_csv(r\"C:\\Users\\HP\\Desktop\\Intern\\Alfido tech\\task_3\\2017PurchasePricesDec.csv\")\n",
    "sales = pd.read_csv(r\"C:\\Users\\HP\\Desktop\\Intern\\Alfido tech\\task_3\\SalesFINAL12312016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80041f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase_prices Columns:\n",
      "['InventoryId', 'Store', 'Brand', 'Description', 'Size', 'VendorNumber', 'VendorName', 'PONumber', 'PODate', 'ReceivingDate', 'InvoiceDate', 'PayDate', 'PurchasePrice', 'Quantity', 'Dollars', 'Classification']\n",
      "\n",
      "beg_inv Columns:\n",
      "['InventoryId', 'Store', 'City', 'Brand', 'Description', 'Size', 'onHand', 'Price', 'startDate']\n",
      "\n",
      "end_inv Columns:\n",
      "['InventoryId', 'Store', 'City', 'Brand', 'Description', 'Size', 'onHand', 'Price', 'endDate']\n",
      "\n",
      "invoice_purchases Columns:\n",
      "['VendorNumber', 'VendorName', 'InvoiceDate', 'PONumber', 'PODate', 'PayDate', 'Quantity', 'Dollars', 'Freight', 'Approval']\n",
      "\n",
      "purchases Columns:\n",
      "['Brand', 'Description', 'Price', 'Size', 'Volume', 'Classification', 'PurchasePrice', 'VendorNumber', 'VendorName']\n",
      "\n",
      "sales Columns:\n",
      "['InventoryId', 'Store', 'Brand', 'Description', 'Size', 'SalesQuantity', 'SalesDollars', 'SalesPrice', 'SalesDate', 'Volume', 'Classification', 'ExciseTax', 'VendorNo', 'VendorName']\n"
     ]
    }
   ],
   "source": [
    "print(\"purchase_prices Columns:\")\n",
    "print(purchase_prices.columns.tolist())\n",
    "\n",
    "print(\"\\nbeg_inv Columns:\")\n",
    "print(beg_inv.columns.tolist())\n",
    "\n",
    "print(\"\\nend_inv Columns:\")\n",
    "print(end_inv.columns.tolist())\n",
    "\n",
    "print(\"\\ninvoice_purchases Columns:\")\n",
    "print(invoice_purchases.columns.tolist())\n",
    "\n",
    "print(\"\\npurchases Columns:\")\n",
    "print(purchases.columns.tolist())\n",
    "\n",
    "print(\"\\nsales Columns:\")\n",
    "print(sales.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa2b40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in purchase_prices:\n",
      "Size    3\n",
      "dtype: int64\n",
      "\n",
      "Missing values in end_inv:\n",
      "City    1284\n",
      "dtype: int64\n",
      "\n",
      "Missing values in purchases:\n",
      "Description    1\n",
      "Size           1\n",
      "Volume         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing data in each dataset\n",
    "datasets = [purchase_prices, beg_inv, end_inv, invoice_purchases, purchases, sales]\n",
    "dataset_names = [\"purchase_prices\", \"beg_inv\", \"end_inv\", \"invoice_purchases\", \"purchases\", \"sales\"]\n",
    "\n",
    "for name, data in zip(dataset_names, datasets):\n",
    "    missing_values = data.isnull().sum()\n",
    "    non_zero_missing_values = missing_values[missing_values > 0]\n",
    "    \n",
    "    if not non_zero_missing_values.empty:\n",
    "        print(f\"\\nMissing values in {name}:\")\n",
    "        print(non_zero_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0bfa1",
   "metadata": {},
   "source": [
    "Handling missing values is a pivotal step in our data analysis journey. Different data segments might demand distinct strategies. Let's collaboratively address each dataset and decide on the most appropriate course of action:\n",
    "\n",
    "purchase_prices:\n",
    "\n",
    "    Description, Size, and Volume: There's a limited number of missing entries here. We have a few options: removing these specific rows, or, for a more informed approach, substituting these gaps with the most commonly observed value (mode). We'll go with removing these rows\n",
    "\n",
    "end_inv:\n",
    "\n",
    "    City: An immediate solution could be to replace the missing 'City' entries with the most frequently occurring city (mode). However, if our 'Store' column has location insights, we might consider using it for a more informed fill. If neither works, marking them as 'Unknown' might be our best bet. We'll try our chances with using 'Store' column.\n",
    "\n",
    "invoice_purchases:\n",
    "\n",
    "    Approval: If we interpret 'Approval' as a binary indicator (e.g., Approved/Not Approved), the absences might hint that the status of these invoices hasn't been decided. We can categorize them as 'Pending' or 'Unknown' for clarity.\n",
    "\n",
    "purchases:\n",
    "\n",
    "    Size: Mirroring our strategy with 'purchase_prices', we can discard these few rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4aca90c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Volume'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Volume'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m cols_to_check \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSize\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_to_check:\n\u001b[1;32m----> 4\u001b[0m     purchase_prices \u001b[38;5;241m=\u001b[39m purchase_prices[purchase_prices[col]\u001b[38;5;241m.\u001b[39mnotna()]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Handling missing values for end_inv dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end_inv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m==\u001b[39m end_inv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Volume'"
     ]
    }
   ],
   "source": [
    "# Handling missing values for purchase_prices dataset\n",
    "cols_to_check = ['Description', 'Size', 'Volume']\n",
    "for col in cols_to_check:\n",
    "    purchase_prices = purchase_prices[purchase_prices[col].notna()]\n",
    "\n",
    "# Handling missing values for end_inv dataset\n",
    "if end_inv['Store'].nunique() == end_inv['City'].nunique():\n",
    "    city_store_mapping = end_inv[['Store', 'City']].drop_duplicates().set_index('Store').to_dict()['City']\n",
    "    end_inv['City'] = end_inv['City'].fillna(end_inv['Store'].map(city_store_mapping))\n",
    "else:\n",
    "    end_inv['City'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Handling missing values for invoice_purchases dataset\n",
    "invoice_purchases['Approval'].fillna('Pending', inplace=True)\n",
    "\n",
    "# Handling missing values for purchases dataset\n",
    "purchases = purchases[purchases['Size'].notna()]\n",
    "\n",
    "datasets = [purchase_prices, beg_inv, end_inv, invoice_purchases, purchases, sales]\n",
    "dataset_names = [\"purchase_prices\", \"beg_inv\", \"end_inv\", \"invoice_purchases\", \"purchases\", \"sales\"]\n",
    "\n",
    "for name, data in zip(dataset_names, datasets):\n",
    "    missing_values = data.isnull().sum()\n",
    "    non_zero_missing_values = missing_values[missing_values > 0]\n",
    "    \n",
    "    if not non_zero_missing_values.empty:\n",
    "        print(f\"\\nMissing values in {name}:\")\n",
    "        print(non_zero_missing_values)\n",
    "    else:\n",
    "        print(f\"\\nNo missing values in {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61083d2",
   "metadata": {},
   "source": [
    "# 2. Inventory Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grouping by Brand and Description and summarize inventory for beginning of the year\n",
    "beg_summary = beg_inv.groupby(['Brand', 'Description'])['onHand'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Grouping by Brand and Description and summarize inventory for end of the year\n",
    "end_summary = end_inv.groupby(['Brand', 'Description'])['onHand'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Identifying top 5 products at the beginning and end of the year\n",
    "top_5_beg = beg_summary.head(5)\n",
    "top_5_end = end_summary.head(5)\n",
    "\n",
    "# Identifying bottom 5 products at the beginning and end of the year\n",
    "bottom_5_beg = beg_summary.tail(5)\n",
    "bottom_5_end = end_summary.tail(5)\n",
    "\n",
    "print(\"Top 5 products at the beginning of the year:\\n\", top_5_beg)\n",
    "print(\"\\nTop 5 products at the end of the year:\\n\", top_5_end)\n",
    "print(\"\\nBottom 5 products at the beginning of the year:\\n\", bottom_5_beg)\n",
    "print(\"\\nBottom 5 products at the end of the year:\\n\", bottom_5_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ff92e",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "Top Products:\n",
    "\n",
    "    \"Ketel One Vodka\" emerged as the top product by the end of the year, even though it was in the 4th position at the beginning. This could imply an increased demand or higher restocking levels for this product during the year.\n",
    "    \"Capt Morgan Spiced Rum\", initially the highest in inventory at the start of the year, narrowly missed the top position by a single unit by the year's end.\n",
    "    \"Smirnoff 80 Proof\" seems to have undergone a brand update or rebranding, as the brand number changed from 3876 to 8111 during the year. Despite this, its popularity remained consistent.\n",
    "    \"Absolut 80 Proof\" held steady, only dropping one rank from the beginning to the end of the year.\n",
    "    A new entrant, \"Jack Daniels No 7 Black\", made its way into the top 5 by the end of the year, replacing \"Maurice's Mentholated Mint\" from the beginning of the year's list.\n",
    "\n",
    "Bottom Products:\n",
    "\n",
    "    It's alarming to see products with zero inventory both at the beginning and the end of the year. It's possible that these items either never had stock during the entire year, or they had stock that was completely sold out and never replenished.\n",
    "    There isn't a significant overlap between the products in the bottom 5 of both lists, indicating that inventory for low-stock items could be fluctuating throughout the year.\n",
    "    The presence of wines and niche products among the bottom lists suggests that these might be specialty items with selective demand or limited supply.\n",
    "\n",
    "Inventory Management Observations:\n",
    "\n",
    "    The overall high consistency among the top products suggests steady demand and effective inventory replenishment strategies for popular items.\n",
    "    However, the consistent zero counts at both year start and end for certain products indicate potential issues in inventory management or procurement. It may be worthwhile to review the demand for these products and determine if it makes business sense to continue carrying them or if there might be supply chain issues preventing their restocking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c11fc",
   "metadata": {},
   "source": [
    "# 3. Sales Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469991e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best-selling products\n",
    "best_selling_products = sales.groupby(['Brand', 'Description']).agg({'SalesQuantity': 'sum'}).sort_values(by='SalesQuantity', ascending=False).head(10)\n",
    "print(f\"Best selling ten products:\\n{best_selling_products}\\n\")\n",
    "\n",
    "# Finding the slow-moving products\n",
    "slow_moving_products = sales.groupby(['Brand', 'Description']).agg({'SalesQuantity': 'sum'}).sort_values(by='SalesQuantity', ascending=True).head(10)\n",
    "print(f\"Slow-moving ten products:\\n{slow_moving_products}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['SalesDate'] = pd.to_datetime(sales['SalesDate'])\n",
    "sales_quantity_trend = sales.groupby('SalesDate').agg({'SalesQuantity': 'sum'})\n",
    "sales_quantity_trend.plot(figsize=(12, 6), title='Sales Quantity Over Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price_trend = sales.groupby('SalesDate').agg({'SalesPrice': 'mean'})\n",
    "avg_price_trend.plot(figsize=(12, 6), title='Average Sales Price Over Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c990cf",
   "metadata": {},
   "source": [
    "There's a cyclical nature to the sales quantities, which might hint at weekly patterns or trends.\n",
    "A significant peak is noticed around January 25th; this could be attributed to specific events, promotions, or even seasonal demands. However, as we transition into February, there's a noticeable decline in sales, which then appears to stabilize as the month progresses.\n",
    "Spirits like \"Smirnoff 80 Proof\" emerge as the top sellers, reflecting a steady demand. On the other hand, several products are moving slowly, indicating limited sales. As these products vary in type and brand, a strategic review might be necessary to decide on their continued stocking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52790381",
   "metadata": {},
   "source": [
    "# 4. Purchasing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa4f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_purchase_volume = invoice_purchases.groupby('VendorName').agg({'Quantity': 'sum'}).sort_values(by='Quantity', ascending=False)\n",
    "print(\"Top 10 Vendors by Purchase Volume:\\n\", vendor_purchase_volume.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1561467",
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_purchase_cost = purchases.groupby('VendorName').agg({'PurchasePrice': 'sum'}).sort_values(by='PurchasePrice', ascending=False)\n",
    "print(\"Top 10 Vendors by Purchase Cost:\\n\", vendor_purchase_cost.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db8c24",
   "metadata": {},
   "source": [
    "Top Vendors by Purchase Cost:\n",
    "\n",
    "    DIAGEO NORTH AMERICA INC stands out as the top vendor with the highest purchase cost, amounting to 3,919,293.52 US dollars.\n",
    "    The following two vendors, 'JIM BEAM BRANDS COMPANY' and 'PERNOD RICARD USA', have notable purchase costs of 2,445,075.37 and 2,002,210.70, respectively.\n",
    "    It's interesting to see that the top 10 vendors have a significant difference in their purchase costs, with 'DIAGEO NORTH AMERICA INC' nearly leading by a margin of 1.5 million usd from the vendor in the second position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ef752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting PODate and ReceivingDate columns to datetime\n",
    "purchases['PoDate'] = pd.to_datetime(purchases['PoDate'], errors='coerce')\n",
    "purchases['ReceivingDate'] = pd.to_datetime(purchases['ReceivingDate'], errors='coerce')\n",
    "\n",
    "# Checking if there are any null values after conversion\n",
    "if purchases['PoDate'].isnull().any() or purchases['ReceivingDate'].isnull().any():\n",
    "    print(\"There are invalid date entries in the dataset. Please review the data.\")\n",
    "else:\n",
    "    purchases['SupplyDuration'] = (purchases['ReceivingDate'] - purchases['PoDate']).dt.days\n",
    "    average_supply_duration = purchases['SupplyDuration'].mean()\n",
    "    print(\"Average Supply Duration (in days):\", average_supply_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb3f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
